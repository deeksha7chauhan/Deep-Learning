{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f4Pq0Kcd7Vq"
      },
      "source": [
        "### Step 1. Importing all important libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "L_gLWFNuZv0e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4NQVLxDeCAH"
      },
      "source": [
        "### Step 2: Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "bkJYiWZebgCw"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"/content/mnist_train.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Data + Normalise [0-1]"
      ],
      "metadata": {
        "id": "6awXhnYPncY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_np = (df_train.iloc[:, 1:].values.astype(\"float32\")) / 255.0\n",
        "y_train_np = df_train.iloc[:, 0].values.astype(\"int64\")\n"
      ],
      "metadata": {
        "id": "kGHMzKCF8eSM"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Data + Normalise"
      ],
      "metadata": {
        "id": "dUyIWAXUnpNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"/content/mnist_test.csv\")\n",
        "X_test_np  = (df_test.iloc[:, 1:].values.astype(\"float32\")) / 255.0\n",
        "y_test_np  = df_test.iloc[:, 0].values.astype(\"int64\")"
      ],
      "metadata": {
        "id": "opJ-PLw9nnQq"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert to \"Torch Tensors\"**"
      ],
      "metadata": {
        "id": "6h7Sn7pQoASg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to torch tensors\n",
        "X_train = torch.from_numpy(X_train_np)          # shape (N, 784)\n",
        "y_train = torch.from_numpy(y_train_np)          # shape (N,)\n",
        "X_test  = torch.from_numpy(X_test_np)\n",
        "y_test  = torch.from_numpy(y_test_np)"
      ],
      "metadata": {
        "id": "dPX2vXN1n_iV"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.view(X_train.shape[0], -1).contiguous()\n",
        "X_test  = X_test.view(X_test.shape[0], -1).contiguous()\n",
        "y_train = y_train.long()\n",
        "y_test  = y_test.long()"
      ],
      "metadata": {
        "id": "U_nQg7uc854m"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_dtype(torch.float32)\n",
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "Fg9VWzFjnejm"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib_h2ailAWbp"
      },
      "source": [
        "### Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i54c-IVkFa1S",
        "outputId": "82367229-2c65-4cd9-e718-b1318c2345a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a3a11b08750>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "3vY-YOL3vZx_"
      },
      "outputs": [],
      "source": [
        "X = np.reshape(X_train, newshape=(X_train.shape[0], 784))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SoftMax**:-transforms a vector of raw model outputs, known as logits, into a probability distribution.\n",
        "- raw outputs from a neural network's final layer are often called \"logits.\"\n",
        "- those values can range upto -ve infinity to + infinity\n",
        "- It takes a set of numbers and converts them into probabilities that sum up to 1.\n",
        "- works on an entire vector of values\n",
        "- typically applied to the final layer of a network designed for multi-class classification.\n",
        "- When we have multiple possible categories and need our model to indicate the probability of each category.\n"
      ],
      "metadata": {
        "id": "Kbp_Jdrdxzhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"label range:\", int(y_train.min()), \"to\", int(y_train.max()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD63V30g91ig",
        "outputId": "be1d1393-776d-4749-d95a-74d8a03be58d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label range: 0 to 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "#mentioned in HW\n",
        "in_dim = X_train.shape[1]\n",
        "hidden = 100\n",
        "out_dim = 10 #classes"
      ],
      "metadata": {
        "id": "pBguba4q-oJB"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = torch.randn(in_dim, hidden) * math.sqrt(2.0 / (in_dim + hidden))\n",
        "b1 = torch.zeros(1, hidden)\n",
        "W2 = torch.randn(hidden, out_dim) * math.sqrt(2.0 / (hidden + out_dim))\n",
        "b2 = torch.zeros(1, out_dim)\n",
        "\n",
        "# keep float32\n",
        "W1 = W1.float(); b1 = b1.float()\n",
        "W2 = W2.float(); b2 = b2.float()"
      ],
      "metadata": {
        "id": "UmEQCyA4-haG"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward Pass**"
      ],
      "metadata": {
        "id": "7G2nt1vx-y0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "3MTaC8Tbq4bH"
      },
      "outputs": [],
      "source": [
        "def iterate_minibatches(X, y, batch_size=256, shuffle=True):\n",
        "    N = X.shape[0]\n",
        "    idx = torch.arange(N)\n",
        "    if shuffle:\n",
        "        idx = idx[torch.randperm(N)]\n",
        "    for s in range(0, N, batch_size):\n",
        "        e = min(s + batch_size, N)\n",
        "        b = idx[s:e]\n",
        "        yield X[b], y[b]\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + torch.exp(-z))\n",
        "\n",
        "def sigmoid_deriv(a):\n",
        "    return a * (1.0 - a)   # if a = sigmoid(z)\n",
        "\n",
        "## keepdim=True; keep the reduced axis in the result (with size 1) instead of removing it.\n",
        "def softmax(z):\n",
        "    zmax = z.max(dim=1, keepdim=True).values\n",
        "    e = torch.exp(z - zmax)\n",
        "    return e / e.sum(dim=1, keepdim=True)\n",
        "\n",
        "def accuracy(probs, y):\n",
        "    return (probs.argmax(dim=1) == y).float().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X):\n",
        "    z1 = X @ W1 + b1          # (B, 100)\n",
        "    a1 = sigmoid(z1)          # hidden activation\n",
        "    z2 = a1 @ W2 + b2         # (B, 10) logits\n",
        "    probs = softmax(z2)       # (B, 10) class probabilities\n",
        "    cache = (X, z1, a1, z2, probs)\n",
        "    return probs, cache\n",
        "\n",
        "# quick sanity check\n",
        "with torch.no_grad():\n",
        "    p, _ = forward(X_train[:4])           # small batch\n",
        "    print(\"forward shapes:\", p.shape)     # expect (4, 10)\n",
        "    print(\"row sums (should be 1):\", p.sum(dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INyRVDIm-uTZ",
        "outputId": "eb24dc0f-a72c-4da1-8330-a937495c3808"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forward shapes: torch.Size([4, 10])\n",
            "row sums (should be 1): tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cross-entropy** (or some loss function) in order to train the network.\n",
        "- forward pass gives you probabilities p.\n",
        "- to update weights, need to know how wrong the predictions are. that’s what the loss does.\n",
        "- in classification with softmax output, the natural choice is cross-entropy loss."
      ],
      "metadata": {
        "id": "FmcZ_HRJ1_u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(y, C=10):\n",
        "    Y = torch.zeros((y.shape[0], C), dtype=torch.float32, device=y.device)\n",
        "    Y.scatter_(1, y.view(-1,1), 1.0)\n",
        "    return Y\n",
        "\n",
        "def cross_entropy_from_probs(probs, y_onehot, eps=1e-12):\n",
        "    p = torch.clamp(probs, eps, 1.0)              # avoid log(0)\n",
        "    return -(y_onehot * torch.log(p)).sum(dim=1).mean()\n",
        "def logsumexp(x, dim=-1, keepdim=False):\n",
        "    m = x.max(dim=dim, keepdim=True).values\n",
        "    out = m + torch.log(torch.sum(torch.exp(x - m), dim=dim, keepdim=True))\n",
        "    return out if keepdim else out.squeeze(dim)\n",
        "\n",
        "def cross_entropy_from_logits(logits, y):\n",
        "    # logits: (B, 10), y: (B,) class indices\n",
        "    logZ = logsumexp(logits, dim=1, keepdim=True)     # (B,1)\n",
        "    log_probs = logits - logZ                         # log-softmax\n",
        "    picked = log_probs.gather(1, y.view(-1,1)).squeeze(1)\n",
        "    return (-picked).mean()\n",
        "\n"
      ],
      "metadata": {
        "id": "QyxyQXrC_SkK"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    probs, _ = forward(X_train[:8])\n",
        "    yoh = one_hot(y_train[:8], C=out_dim)\n",
        "    loss = cross_entropy_from_probs(probs, yoh)\n",
        "    print(f\"loss (sanity): {float(loss):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Byc0m2v_UmV",
        "outputId": "2f0c4246-10f2-452a-9139-2fb5f0fe08a9"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss (sanity): 2.4021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backward Pass**"
      ],
      "metadata": {
        "id": "22F4EMMx_w98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(cache, y):\n",
        "    X, z1, a1, z2, probs = cache\n",
        "    B = y.shape[0]\n",
        "    Y = one_hot(y, C=10)                 # (B, 10)\n",
        "    dz2 = (probs - Y) / B                # (B, 10)\n",
        "    dW2 = a1.T @ dz2                     # (100, 10)\n",
        "    db2 = dz2.sum(dim=0, keepdim=True)   # (1, 10)\n",
        "    da1 = dz2 @ W2.T                     # (B, 100)\n",
        "    dz1 = da1 * (a1 * (1.0 - a1))        # sigmoid_deriv(a1)\n",
        "    dW1 = X.T @ dz1                      # (784, 100)\n",
        "    db1 = dz1.sum(dim=0, keepdim=True)   # (1, 100)\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "probs, cache = forward(X_train[:32])\n",
        "grads = backward(cache, y_train[:32])\n",
        "print([g.shape for g in grads])  # expect [(784,100), (1,100), (100,10), (1,10)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Few-eVoAIKn",
        "outputId": "96b97a1b-f713-4a34-c131-0c69e7f825e2"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.Size([784, 100]), torch.Size([1, 100]), torch.Size([100, 10]), torch.Size([1, 10])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD"
      ],
      "metadata": {
        "id": "PmCPW_A6Ache"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step(grads, lr):\n",
        "    global W1, b1, W2, b2\n",
        "    dW1, db1, dW2, db2 = grads\n",
        "    with torch.no_grad():\n",
        "        W1 -= lr * dW1\n",
        "        b1 -= lr * db1\n",
        "        W2 -= lr * dW2\n",
        "        b2 -= lr * db2"
      ],
      "metadata": {
        "id": "hlgJKUskAb-l"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy"
      ],
      "metadata": {
        "id": "YSpSPNGw559h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy\n",
        "@torch.no_grad()\n",
        "def accuracy(X, y):\n",
        "    probs, _ = forward(X)\n",
        "    pred = probs.argmax(dim=1)\n",
        "    return (pred == y).float().mean().item()\n",
        "\n",
        "# simple minibatch iterator\n",
        "def iterate_minibatches(X, y, batch_size=256, shuffle=True):\n",
        "    N = X.shape[0]\n",
        "    idx = torch.arange(N)\n",
        "    if shuffle:\n",
        "        idx = idx[torch.randperm(N)]\n",
        "    for s in range(0, N, batch_size):\n",
        "        e = min(s + batch_size, N)\n",
        "        b = idx[s:e]\n",
        "        yield X[b], y[b]\n",
        "\n",
        "# hyperparameters\n",
        "lr = 0.05\n",
        "epochs = 20\n",
        "batch_size = 256\n",
        "\n",
        "print(\"training 784→100→10 …\")\n",
        "for ep in range(1, epochs + 1):\n",
        "    total_loss, batches = 0.0, 0\n",
        "    for xb, yb in iterate_minibatches(X_train, y_train, batch_size=batch_size, shuffle=True):\n",
        "        probs, cache = forward(xb)\n",
        "        y_onehot = one_hot(yb, C=10)\n",
        "        loss = cross_entropy_from_probs(probs, y_onehot)\n",
        "        grads = backward(cache, yb)\n",
        "        step(grads, lr)\n",
        "        total_loss += float(loss); batches += 1\n",
        "\n",
        "    tr_acc = accuracy(X_train, y_train)\n",
        "    te_acc = accuracy(X_test,  y_test)\n",
        "    print(f\"Epoch {ep:02d} | loss={total_loss/max(1,batches):.3f} | \"\n",
        "          f\"train_acc={tr_acc:.3f} | test_acc={te_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na4iLdR6AvlP",
        "outputId": "965fb74f-bd52-46d1-f82b-8a377e9ee212"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training 784→100→10 …\n",
            "Epoch 01 | loss=0.209 | train_acc=0.941 | test_acc=0.940\n",
            "Epoch 02 | loss=0.207 | train_acc=0.942 | test_acc=0.940\n",
            "Epoch 03 | loss=0.205 | train_acc=0.943 | test_acc=0.941\n",
            "Epoch 04 | loss=0.204 | train_acc=0.943 | test_acc=0.941\n",
            "Epoch 05 | loss=0.202 | train_acc=0.944 | test_acc=0.942\n",
            "Epoch 06 | loss=0.201 | train_acc=0.944 | test_acc=0.942\n",
            "Epoch 07 | loss=0.199 | train_acc=0.945 | test_acc=0.942\n",
            "Epoch 08 | loss=0.198 | train_acc=0.945 | test_acc=0.943\n",
            "Epoch 09 | loss=0.197 | train_acc=0.945 | test_acc=0.943\n",
            "Epoch 10 | loss=0.195 | train_acc=0.945 | test_acc=0.943\n",
            "Epoch 11 | loss=0.194 | train_acc=0.946 | test_acc=0.944\n",
            "Epoch 12 | loss=0.193 | train_acc=0.946 | test_acc=0.944\n",
            "Epoch 13 | loss=0.191 | train_acc=0.946 | test_acc=0.944\n",
            "Epoch 14 | loss=0.190 | train_acc=0.947 | test_acc=0.944\n",
            "Epoch 15 | loss=0.189 | train_acc=0.947 | test_acc=0.945\n",
            "Epoch 16 | loss=0.187 | train_acc=0.947 | test_acc=0.945\n",
            "Epoch 17 | loss=0.186 | train_acc=0.948 | test_acc=0.946\n",
            "Epoch 18 | loss=0.184 | train_acc=0.949 | test_acc=0.946\n",
            "Epoch 19 | loss=0.183 | train_acc=0.949 | test_acc=0.947\n",
            "Epoch 20 | loss=0.182 | train_acc=0.949 | test_acc=0.947\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}